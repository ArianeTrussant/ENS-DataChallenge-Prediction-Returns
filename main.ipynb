{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nW4CnQ34c36"
   },
   "source": [
    "This project aims to predict the direction of a stock's price during the last two hours of trading (2 PM - 4 PM), using data from the beginning of the day. 2 PM - 4 PM is the most liquid period, with significant trading activity. Predicting price movements is useful to optimize a portfolio by strategically adjusting the positions.\n",
    "\n",
    "The three possible directions:\n",
    "- Sharp price decline\n",
    "- Minor change (little variation, regardless of direction)\n",
    "- Sharp price increase\n",
    "\n",
    "The objective is to use predictive models to anticipate these movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpfkX8skqOFE"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKVP3diGzUAY",
    "outputId": "fa37d104-6ce7-46c4-8cfb-855c9b4e734b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.stats import skew, kurtosis, chi2_contingency, kruskal\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import hdbscan\n",
    "import os\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJOulCLOqRfA"
   },
   "outputs": [],
   "source": [
    "input_train = pd.read_csv(\"data/input_training.csv\")\n",
    "output_train = pd.read_csv(\"data/output_training_gmEd6Zt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mQz7M7wwFwi"
   },
   "outputs": [],
   "source": [
    " # merge X and Y in the same dataframe\n",
    "data = input_train.merge(output_train, on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NTDcU8vxci4"
   },
   "source": [
    "The data :\n",
    "- Returns. These have a 5-minute granularity. For example, r0: return between the 9:30 and 9:35 prices. Up to r52: return between the 1:55 and 2:00 prices. Returns are calculated in basis points.\n",
    "- Day: ID of the day corresponding to a stock's intraday returns. Important: no days from the training set are included in the test set to avoid information leaks. We have 502 days, or approximately 2 years of trading data.\n",
    "- Equity: ID of the stock in question. Similarly, no stock from the training set is found in the test set.\n",
    "\n",
    "Reod : target variable\n",
    "- if the yield calculated in basis points between 2 p.m. and 4 p.m. is less than -25 bps: reod takes the value -1\n",
    "- Between -25 and 25 bps: takes the value 0\n",
    "- Greater than 25 bps: takes the value 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omrllEIOtATg"
   },
   "source": [
    "# Cleaning and significance testing of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2gG2N5z-gy2"
   },
   "source": [
    "## Processing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24Up07Xx9nk7",
    "outputId": "ea8377de-f989-4919-8b27-21cb68e7ef25"
   },
   "outputs": [],
   "source": [
    "#number of NA by column\n",
    "print(data.isna().sum())\n",
    "# percentage of NAN by column\n",
    "print(data.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdDy2qHF2Y5V",
    "outputId": "4896a6a4-50fd-47ed-8204-a5427e9b80fb"
   },
   "outputs": [],
   "source": [
    "print(data.isna().sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q3_-4hC9y4d"
   },
   "source": [
    "Missing data are only found in the returns. We delete lines where more than 30% of returns are missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpIUDV7E9OrV"
   },
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0, thresh = data.shape[1] * 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2eAyDdy-COM"
   },
   "source": [
    "When a return is missing, we replace it by the last return available before. If the first return is missing, we replace it by the next return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jwaxZykdeP-"
   },
   "outputs": [],
   "source": [
    "# colonnes de rendements\n",
    "return_cols = [col for col in data.columns if col.startswith('r') and col != 'reod']\n",
    "data[return_cols] = data[return_cols].bfill(axis=0).ffill(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuLCY_qxrPhX"
   },
   "source": [
    "Show the repartition of values in Reod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TpbC35wJrH9T",
    "outputId": "4c39bfd8-1ccf-49eb-db5f-dba12b186a45"
   },
   "outputs": [],
   "source": [
    "value_counts = data['reod'].value_counts()\n",
    "value_percentages = (value_counts / len(data)) * 100\n",
    "\n",
    "for value, count in value_counts.items():\n",
    "    percentage = value_percentages[value]\n",
    "    print(f\"Value : {value} | Number : {count} | Percentage : {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3p-1s4EerToU"
   },
   "source": [
    "The values ​​are roughly evenly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Skt2HZHatATn"
   },
   "source": [
    "## Extreme values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqgK1DcmtATn"
   },
   "source": [
    "We need to check for extreme or even outlier values ​​in the data, specifically in the stocks returns.\n",
    "\n",
    "We look at how many returns are >100bp or <100bp, which corresponds to an increase/decrease of more than 1% in 5 minutes.\n",
    "\n",
    "We also look at the minimum and maximum values ​​to see if they are high but economically consistent, or if there may be any outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEchSvcztATn",
    "outputId": "11865755-b545-485a-8567-761f65e2356b"
   },
   "outputs": [],
   "source": [
    "# returns\n",
    "rendement_cols = [col for col in data.columns if col.startswith(\"r\") and col != \"reod\"]\n",
    "\n",
    "# put them in a serie\n",
    "stacked_rendements = (data[[\"ID\"] + rendement_cols].astype({col: \"float32\" for col in rendement_cols})\n",
    "                      .set_index(\"ID\").stack().reset_index())\n",
    "stacked_rendements.columns = [\"ID\", \"colonne\", \"valeur\"]\n",
    "\n",
    "# we group them according to < 100 ; > 100 or between -100 and 100\n",
    "rendements = stacked_rendements[\"valeur\"]\n",
    "groupe_A = rendements[(rendements >= -100) & (rendements <= 100)].count()\n",
    "groupe_B = rendements[rendements < -100].count()\n",
    "groupe_C = rendements[rendements > 100].count()\n",
    "total = len(rendements)\n",
    "\n",
    "print(\"Repartition of returns (bps) :\")\n",
    "print(f\"Returns between [-100 ; 100] : {groupe_A} observations ({groupe_A/total:.2%})\")\n",
    "print(f\"Returns < -100: {groupe_B} observations ({groupe_B/total:.2%})\")\n",
    "print(f\"Returns > 100: {groupe_C} observations ({groupe_C/total:.2%})\")\n",
    "print(f\"Extreme returns : {(groupe_B + groupe_C)/total:.2%}\")\n",
    "\n",
    "# top 5 of highest and lowest returns\n",
    "top_5 = stacked_rendements.sort_values(\"valeur\", ascending=False).head(5)\n",
    "bottom_5 = stacked_rendements.sort_values(\"valeur\", ascending=True).head(5)\n",
    "print(\"\\n 5 highest returns :\")\n",
    "print(top_5)\n",
    "print(\"\\n 5 lowest returns :\")\n",
    "print(bottom_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDIuiAhPtATn"
   },
   "source": [
    "The percentage of extreme returns is correct : this could correspond to days of announcements, publications, market stress, etc. However, we have some outlier extreme values ​​that need to be addressed.\n",
    "\n",
    "We will remove all returns exceeding a variation of more than 15%. This gives us enough margin to still account for large market movements, while remaining within the limits of what is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh7AfqkclGwV"
   },
   "outputs": [],
   "source": [
    "#delete extreme returns\n",
    "data[rendement_cols] = data[rendement_cols].where((data[rendement_cols] >= -1500) & (data[rendement_cols] <= 1500))\n",
    "\n",
    "# replace NAN by the previous value if it exists\n",
    "data[rendement_cols] = data[rendement_cols].apply(lambda row: row.ffill(), axis=1)\n",
    "\n",
    "# for NAN returns without a previous value, we replace them by the next return\n",
    "data[rendement_cols] = data[rendement_cols].apply(lambda row: row.bfill(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0r2upkOtATn"
   },
   "source": [
    "Plot the returns distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3F_XmPUatATo",
    "outputId": "7f75bf4f-117c-4f47-f935-ad018835046d"
   },
   "outputs": [],
   "source": [
    "rendements_serie = data[rendement_cols].stack().dropna()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(rendements_serie, bins=200, edgecolor='black')\n",
    "plt.title(\"Distribution of returns (log scale)\")\n",
    "plt.xlabel(\"Return (bps)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RuVKzR2StATo",
    "outputId": "0492c1c4-7253-4050-8991-e26e2eeb4bdd"
   },
   "outputs": [],
   "source": [
    "mean_val = rendements_serie.mean()\n",
    "std_val = rendements_serie.std()\n",
    "skew_val = skew(rendements_serie)\n",
    "kurt_val = kurtosis(rendements_serie, fisher=False)\n",
    "\n",
    "print(\"\\Statistical moments of returns (en bps) :\")\n",
    "print(f\"Mean: {mean_val:.2f}\")\n",
    "print(f\"Standard deviation: {std_val:.2f}\")\n",
    "print(f\"Skewness: {skew_val:.4f}\")\n",
    "print(f\"Kurtosis: {kurt_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean is almost null, which means the distribution is almost centered.\n",
    "- The standard deviation represents reasonable volatility (0.3452% in 5 minutes). This is consistent for such short time horizons.\n",
    "- The skew is negative, so there is a strong left asymmetry, which means that large declines are more frequent than large increases. This is normal because the market always falls much faster than it rises (panic effect).\n",
    "- The kurtosis is very positive: the distribution tails are much thicker than those of a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO7vsC77tATo"
   },
   "source": [
    "## Correlation between the features and the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLjNS-hCtATo"
   },
   "source": [
    "- For continuous features (returns), we perform a Krustal-Wallis test to verify whether the distribution of returns varies significantly between the classes of reod.\n",
    "- For categorical features (equity, day), we perform a Chi² test to verify whether the distribution of the categorical feature is significantly different between reod classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tZhAn-JtATo"
   },
   "source": [
    "### Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYAuF3xYwLm3"
   },
   "source": [
    "Test Krustal Wallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7J7v-7bJtATo",
    "outputId": "b771e1f2-f528-4588-847d-a85faa6c79e8"
   },
   "outputs": [],
   "source": [
    "features = data.columns[3:-1]  #r0 to r52\n",
    "p_values = {}\n",
    "\n",
    "for feature in features:\n",
    "    groups = [data[data['reod'] == cat][feature] for cat in data['reod'].unique()]\n",
    "    stat, p = kruskal(*groups)\n",
    "    p_values[feature] = p\n",
    "\n",
    "# non significant features (p >= 0.05)\n",
    "non_significant_features = [k for k, v in p_values.items() if v is None or v >= 0.05]\n",
    "\n",
    "print(\"Non significant features (p ≥ 0.05) :\")\n",
    "print(non_significant_features)\n",
    "\n",
    "# sort the most significant features\n",
    "sorted_features = sorted(p_values.items(), key=lambda x: x[1])\n",
    "print(\"10 most correlated features :\")\n",
    "print(sorted_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j6_iU9gtATp"
   },
   "source": [
    "All returns are significant (except two), which mean they influence the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_CveGqitATp"
   },
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpJ5K-AUtATp",
    "outputId": "9c46f82c-ad8f-44c7-bcb5-31ba95e97b83"
   },
   "outputs": [],
   "source": [
    "#contingency table (frequency of reod for each day)\n",
    "contingency_table = pd.crosstab(data['day'], data['reod'])\n",
    "\n",
    "# Test Chi2\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Stat Chi² : {chi2:.4f}\")\n",
    "print(f\"P-value : {p:.8f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"`day` significantly influences `reod`\")\n",
    "else:\n",
    "    print(\"No significant relationship between `day` and `reod`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz4up_ALtATp"
   },
   "source": [
    "According to the Chi-square test, we reject hypothesis H0 with certainty. The equity variable significantly influences the target variable. However, we can note that the p-value is zero, which is strange. This could potentially be explained by the fact that we have a large number of variables, so the Chi-square test is less robust. We will use Cramer's V to confirm the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYlkZ8nktATp",
    "outputId": "72a504e5-c812-4b92-de11-ca421fe12be9"
   },
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "    n = contingency_table.sum().sum()\n",
    "    r, k = contingency_table.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "# Cramer's V bewteen  day and reod\n",
    "v_cramer = cramers_v(data['day'], data['reod'])\n",
    "print(f\" V de Cramer entre `day` et `reod` : {v_cramer:.4f}\")\n",
    "\n",
    "# Cramer's V beetween equity and reod\n",
    "v_cramer_equity = cramers_v(data['equity'], data['reod'])\n",
    "print(f\" V de Cramer entre `equity` et `reod` : {v_cramer_equity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMG6wtFrtATp"
   },
   "source": [
    "There is a significant correlation between the day and the target variable, and between equity and the target variable.\n",
    "\n",
    "We can also group the equity variables according to the reod value most associated with them. We then analyze the extent to which the reod variable depends on the equity type to which it belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIUxld4btATp",
    "outputId": "af0d7ffb-613a-4f5d-e18b-315c52bf4b16"
   },
   "outputs": [],
   "source": [
    "# find the most frequent reod for each equity\n",
    "dominant_reod_per_equity = data.groupby('equity')['reod'].agg(lambda x: x.value_counts().idxmax())\n",
    "equity_reod_group_series = data['equity'].map(dominant_reod_per_equity)\n",
    "\n",
    "# Cramer's V between this serie and the true variable 'reod'\n",
    "v_cramer = cramers_v(equity_reod_group_series, data['reod'])\n",
    "print(f\"Cramer's V after regrouping : {v_cramer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDSs-M_JtATp"
   },
   "source": [
    "Cramer's V is quite low for the equity variable. This may be explained by the fact that equity is not a \"true\" categorical variable. Later in the project, we will use unsupervised training to create equity clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFtwMCzg2lC-"
   },
   "source": [
    "# Benchmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHHOkcno3AoD"
   },
   "source": [
    "We don't have a detailed description of how the challenge benchmark is calculated. We know that it's based on the creation of features based on returns r0 to r52, that it compares the behavior of different stocks and days, and that it uses a basic classifier based on the state of the tree. They achieved a score of 41.74%. The challenge winner achieved 51.93%.\n",
    "\n",
    "We test a naive benchmark:\n",
    "- we look at whether, between 9:30 a.m. and 2:00 p.m., the stock rose 25bp, fell 25bp, or remained stable. We then classify them based on this into +1, -1, and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvSJ4UjVUcB2",
    "outputId": "61697782-3f3a-43b4-ef44-a5889294af4e"
   },
   "outputs": [],
   "source": [
    "benchmark = data.copy()\n",
    "\n",
    "cols_to_keep = [\"ID\", \"day\", \"equity\", \"r0\", \"r52\", 'reod']\n",
    "benchmark = benchmark[cols_to_keep]\n",
    "benchmark[\"reod_estimated\"] = benchmark[\"r52\"] - benchmark[\"r0\"]\n",
    "benchmark[\"reod_estimated\"] = benchmark[\"reod_estimated\"].apply(lambda x: 1 if x > 25 else (-1 if x < -25 else 0))\n",
    "\n",
    "total_percentage = (((benchmark[\"reod\"] == benchmark[\"reod_estimated\"]).sum()) / len(benchmark)) * 100\n",
    "\n",
    "print(f\"Percentage of identical values : {total_percentage:.2f}%\")\n",
    "benchmark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLzKX2UJtYZY"
   },
   "source": [
    "We find a benchmark slightly lower than the one given in the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQayb0jyBBIC"
   },
   "source": [
    "# Creating new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5wYzMn9DdU5"
   },
   "source": [
    "Create multiple features to better analyze the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DidmXHhZtUuB"
   },
   "source": [
    "## New features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YiwsE4utUuC"
   },
   "source": [
    "We begin by creating features derived from standard statistics:\n",
    "- mean, median of returns over a day\n",
    "- standard deviation and variance of returns over a day\n",
    "- amplitude (difference between the maximum and minimum returns for the day).\n",
    "\n",
    "These calculations are applied by equity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMSszS4NtUuC"
   },
   "outputs": [],
   "source": [
    "data['mean_return'] = data.iloc[:, 3:-1].mean(axis=1)\n",
    "data['median_return'] = data.iloc[:, 3:-1].median(axis=1)\n",
    "data['std_return'] = data.iloc[:, 3:-1].std(axis=1)\n",
    "data['var_return'] = data.iloc[:, 3:-1].var(axis=1)\n",
    "data['range_return'] = data.iloc[:, 3:-1].max(axis=1) - data.iloc[:, 3:-1].min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81-vK5uRtUuD"
   },
   "source": [
    "We then calculate:\n",
    "- the dynamics of returns: their trend at the start and end of the session, then we calculate the difference between the trends. (for each day)\n",
    "\n",
    "- the momentum over 10min and 20min. If it is > 0, this means that the stock tends to rise in the last minutes before 2 p.m. (for each day)\n",
    "\n",
    "- the acceleration, to determine whether the stock tends to pick up speed or slow down at the end of the session. (for each day)\n",
    "\n",
    "- the volatility ratio. We compare recent volatility to high volatility. If volatility has recently increased, it means that the market is more unstable towards the end of the session. (for each day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xp6MN091tUuD"
   },
   "outputs": [],
   "source": [
    "data['trend_last_5'] = data[['r48', 'r49', 'r50', 'r51', 'r52']].mean(axis=1)\n",
    "data['trend_first_5'] = data[['r0', 'r1', 'r2', 'r3', 'r4']].mean(axis=1)\n",
    "data['trend_diff'] = data['trend_last_5'] - data['trend_first_5']\n",
    "\n",
    "data['momentum_10'] = data['r52'] - data['r42']\n",
    "data['momentum_20'] = data['r52'] - data['r32']\n",
    "data['acceleration'] = data['momentum_10'] - data['momentum_20']\n",
    "\n",
    "#we look at the volatility of the last half hour before 2 p.m., compared to the total volatility of the day\n",
    "vol_fin = data[['r47', 'r48', 'r49', 'r50', 'r51', 'r52']].std(axis=1)\n",
    "vol_totale = data['std_return']\n",
    "\n",
    "#if all the returns from r47 to r52 are 0, and the standard deviation is 0 -> we generate a NAN. we replace with 0\n",
    "data['volatility_ratio'] = np.where((vol_fin == 0) & (vol_totale == 0), 0.0, vol_fin / vol_totale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPfFUvk7tUuE"
   },
   "source": [
    "The market has cycles (start/end of month effect, quarter, etc.) that can be captured with a sinusoidal transformation on the day data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0ayrMgCtUuE"
   },
   "outputs": [],
   "source": [
    "data['day_sin'] = np.sin(2 * np.pi * data['day'] / max(data['day']))\n",
    "data['day_cos'] = np.cos(2 * np.pi * data['day'] / max(data['day']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64zK9gNntUuE"
   },
   "source": [
    "We can also transform \"day\" into time categories to capture period effects. We divide the \"day\" variable into equal quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzK8Vyl3tUuF"
   },
   "outputs": [],
   "source": [
    "data['day_bin'] = pd.qcut(data['day'], q=10, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZFtI50ztUuF"
   },
   "source": [
    "We now calculate cross-temporal features, i.e., across several days of the sample.\n",
    "\n",
    "For each equity, we calculate:\n",
    "- the moving average of its 5-day and 10-day returns\n",
    "- the rolling volatility over the same intervals\n",
    "- the momentum over the same intervals (the difference between the average returns of the two days forming the interval).\n",
    "- the volatility ratio: the ratio between the 5-day rolling volatility and the 10-day rolling volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iDn3WEocDyN"
   },
   "source": [
    "Check if some equities have less than 11 days of observations, because we could not apply a 10-day rolling window on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "my24R6asaXkY",
    "outputId": "d0fc2a62-2e61-4d17-ce12-dea1a05c05e4"
   },
   "outputs": [],
   "source": [
    "equity_sizes = data['equity'].value_counts()\n",
    "equity_to_remove = equity_sizes[equity_sizes < 11]\n",
    "n_to_drop = equity_to_remove.sum()\n",
    "\n",
    "print(f\"Equity with <11 observations: {len(equity_to_remove)}\")\n",
    "print(f\"Number of rows to delete : {n_to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luBJI3BVcXPO"
   },
   "source": [
    "There are very few equities that have less than 11 observations, and in total that represents 137 lines which is very few given the dataset, so we can afford to delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNiv6Trmb01O"
   },
   "outputs": [],
   "source": [
    "equity_supprimer = equity_sizes[equity_sizes < 11].index\n",
    "data = data[~data['equity'].isin(equity_supprimer)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "766RK1Gxg_UQ"
   },
   "outputs": [],
   "source": [
    "# returns columns\n",
    "return_cols = [col for col in data.columns if col.startswith(\"r\") and col != \"reod\"]\n",
    "\n",
    "# sort by equity and day\n",
    "data = data.sort_values(by=['equity', 'day'])\n",
    "\n",
    "# features rolling\n",
    "for equity in data['equity'].unique():\n",
    "    mask = data['equity'] == equity\n",
    "\n",
    "    mean_return = data.loc[mask, 'mean_return']\n",
    "    std_return = data.loc[mask, 'std_return']\n",
    "\n",
    "    # Moving average\n",
    "    data.loc[mask, 'Moyenne mobile 5d'] = mean_return.rolling(window=5).mean().shift(1)\n",
    "    data.loc[mask, 'Moyenne mobile 10d'] = mean_return.rolling(window=10).mean().shift(1)\n",
    "\n",
    "    # Moving volatility\n",
    "    vol5 = std_return.rolling(window=5).mean().shift(1)\n",
    "    vol10 = std_return.rolling(window=10).mean().shift(1)\n",
    "\n",
    "    data.loc[mask, 'Vol mobile 5d'] = vol5\n",
    "    data.loc[mask, 'Vol mobile 10d'] = vol10\n",
    "\n",
    "    # Momentum\n",
    "    data.loc[mask, 'Momentum mobile 5d'] = (mean_return - mean_return.shift(5)).shift(1)\n",
    "    data.loc[mask, 'Momentum mobile 10d'] = (mean_return - mean_return.shift(10)).shift(1)\n",
    "\n",
    "    # Vol ratio 10d\n",
    "    vol_ratio = np.where((vol5 == 0) & (vol10 == 0), 0.0, #if division 0/0 then = 0\n",
    "        np.where( vol10 == 0,np.nan, vol5 / vol10)) # if numerator / 0 then = NAN\n",
    "    data.loc[mask, 'Vol ratio 10d'] = vol_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TK33_MIDtUuI"
   },
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data = data.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_WDKItU7xjF"
   },
   "source": [
    "# Construction of features with clustering (unsupervised model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvcq8m-TeZNG"
   },
   "source": [
    "## Equities cluster on features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gpa4zDV7xjJ"
   },
   "source": [
    "Previously, we tried to consider equity stocks as categorical features in order to analyze their impact on the target variable, reod. However, equity stocks are not strictly speaking categorical features. To properly analyze their impact on the target variable, we will create equity clusters to group them according to their common characteristics and see if belonging to a certain equity group has an influence on the target variable.\n",
    "\n",
    "We will group the equity stocks according to certain previously created features that seem relevant for grouping the stocks and analyzing their impact on reod.\n",
    "\n",
    "Each cluster represents a type of equity behavior. The goal is to be able to say: \"This group of stocks has a specific trading pattern, so when a new equity exhibits this same pattern, I can predict that it will likely exhibit this type of behavior (e.g., a sharp rise at the end of the session).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ed6l8-vr7xjK"
   },
   "source": [
    "We start by doing a PCA on the features that we find interesting for grouping equity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "Ff-91KPY7xjL",
    "outputId": "eedfceab-ee46-4645-aeb3-fa8d4bcc1342"
   },
   "outputs": [],
   "source": [
    "# relevant features to group equities\n",
    "features_clustering = ['mean_return', 'std_return', 'var_return', 'momentum_10', 'momentum_20',\n",
    "                      'acceleration', 'volatility_ratio', 'day_sin', 'day_cos', 'day_bin',\n",
    "                      'Moyenne mobile 5d', 'Moyenne mobile 10d', 'Vol mobile 5d', 'Vol mobile 10d',\n",
    "                       'Momentum mobile 5d', 'Momentum mobile 10d', 'Vol ratio 10d']\n",
    "\n",
    "# normalize the data in an economically meaningful way\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(data[features_clustering])\n",
    "\n",
    "# PCA\n",
    "pca_full = PCA()\n",
    "X_train_pca = pca_full.fit_transform(X_train_scaled)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# explained variance and scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "axes[0].scatter(X_train_pca[:, 0], X_train_pca[:, 1], marker=\"x\")\n",
    "axes[0].set_title(\"PCA\")\n",
    "axes[0].set_xlabel('Principal Component 1')\n",
    "axes[0].set_ylabel('Principal Component 2')\n",
    "\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title(\"Cumulative Explained Variance by Component\")\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZvL9zvJ7xjL"
   },
   "source": [
    "90% of the variance can be explained by 7 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n8yRgt47xjM"
   },
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8coMRHb27xjM"
   },
   "source": [
    "Group the equity according to certain features that seem interesting (the same as above) and apply an HDBSCAN model to calculate the optimal number of clusters and how to distribute the equity by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mti_RwY47xjM",
    "outputId": "890962e1-017e-47f5-9366-b1870aa0d57e"
   },
   "outputs": [],
   "source": [
    "# aggregate selected features using: mean, std, 75th percentile and 25th percentile\n",
    "aggregated = data.groupby('equity')[features_clustering].agg(['mean', 'std', lambda x: x.quantile(0.25),\n",
    "                                                              lambda x: x.quantile(0.75)])\n",
    "\n",
    "# rename columns properly\n",
    "aggregated.columns = [f\"{col[0]}_{col[1] if isinstance(col[1], str) else 'q'+str(int(col[1](None)*100))}\"\n",
    "                      for col in aggregated.columns]\n",
    "std_cols = [col for col in aggregated.columns if '_std' in col]\n",
    "aggregated[std_cols] = aggregated[std_cols].fillna(0)\n",
    "\n",
    "# normalization in an economically meaningful way\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(aggregated)\n",
    "\n",
    "# create clusters\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "aggregated['cluster'] = clusterer.fit_predict(X_scaled)\n",
    "print(\"Number of equities per cluster:\")\n",
    "print(aggregated['cluster'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7luUCZJ7xjN"
   },
   "source": [
    "- The vast majority of equities are grouped in cluster 2. Cluster 3 may also be interesting given the number of equities within it.\n",
    "- According to the HDBSCAN documentation, \"any point not in a selected cluster is simply a noise point (and assigned the label -1).\" This means that 286 points are not well classified.\n",
    "- We will verify these results with K-means clustering to be able to interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Av_sLEix7xjN"
   },
   "source": [
    "Visualization of our clustering: we use a PCA to reduce the dimension of the problem and be able to visualize our clusters in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "MV1o6e6f7xjN",
    "outputId": "5efc4f0f-59e0-4fae-dfc6-3ce42c016e69"
   },
   "outputs": [],
   "source": [
    "# ACP\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "aggregated['PCA1'] = X_pca[:, 0]\n",
    "aggregated['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "# plot clustering\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(data=aggregated, x='PCA1', y='PCA2', hue='cluster', palette='tab10', s=60)\n",
    "plt.title(\"Clustering with HDBSCAN\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGHS20j97xjO"
   },
   "source": [
    "- Except for group 0, the clusters overlap, which isn't very conclusive.\n",
    "- Finally, group 3 isn't necessarily very interesting because it's very close to group 2. It's more likely group 0 that's interesting because it's very separate from the rest, so the clustering works well (we do have a very different trading pattern for these stocks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPn9xoA87xjO"
   },
   "source": [
    "#### KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVeo3SfEKGV8"
   },
   "source": [
    "We cluster following the same logic, but this time using a K means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEWowa4h7xjO"
   },
   "outputs": [],
   "source": [
    "available_features = [f for f in features_clustering if f in data.columns]\n",
    "# mean by equity\n",
    "equity_features = data.groupby(\"equity\")[available_features].mean().reset_index()\n",
    "\n",
    "# normalize data using the right method\n",
    "X = equity_features.drop(columns=[\"equity\"])\n",
    "scaler = RobustScaler()\n",
    "X_scaled_Kmeans = scaler.fit_transform(X)\n",
    "\n",
    "# ACP\n",
    "pca_kmeans = PCA(n_components=9)\n",
    "X_pca = pca_kmeans.fit_transform(X_scaled_Kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap2EAuku7xjP"
   },
   "source": [
    "The optimal number of clusters is checked using the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "RrkcbU7A7xjP",
    "outputId": "168412bc-a1e7-415e-c25d-71a847211f73"
   },
   "outputs": [],
   "source": [
    "# elbow method\n",
    "inertia = []\n",
    "K = range(1, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_pca)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K, inertia, marker='o')\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Inertia intra-cluster\")\n",
    "plt.title(\"Elbow method (KMeans after PCA)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "fJdwcqjn5k33",
    "outputId": "638432c4-ac57-42b0-aa2c-cf6f44ca349f"
   },
   "outputs": [],
   "source": [
    "# creation of the clusters\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)  # number of clusters = 4, given by the elbow method\n",
    "clusters = kmeans.fit_predict(X_pca)            #clusters on PCA\n",
    "equity_features['cluster'] = clusters\n",
    "\n",
    "# visualise\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette='tab10', s=60)\n",
    "plt.title(\"KMeans Clusyers\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELGVxshJ7xjQ",
    "outputId": "632119d3-e25f-44fe-842a-2cb4d74303fa"
   },
   "outputs": [],
   "source": [
    "compter_equity = equity_features['cluster'].value_counts().sort_index()\n",
    "pourcentage_equity = (compter_equity / compter_equity.sum()) * 100\n",
    "resultat_equity= pd.DataFrame({'Number of equities': compter_equity, \"Percentage(%)\": pourcentage_equity.round(2)})\n",
    "print(resultat_equity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwcsmlNN7xjQ"
   },
   "source": [
    "- The vast majority of equities are concentrated in a cluster. This means that the equities in the dataset behave similarly. Economically, this means that the vast majority of stocks in the US market perform similarly, which isn't necessarily surprising.\n",
    "\n",
    "- The four clusters are still well separated on the graph, which means they clearly explain different trading behaviors. Clustering works better than with HDBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIyhq7If7xjR"
   },
   "source": [
    "Now that we've categorized the equities into clusters, we add this information as a feature to our dataset. We chose to use the clusters created by Kmeans because they are more conclusive than those created by HDBSCAN and seem to provide more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wF1DjW27xjR"
   },
   "outputs": [],
   "source": [
    "data = data.merge(equity_features[['equity', 'cluster']], on='equity', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e7dNDmM7xjR"
   },
   "source": [
    "## Equities cluster on returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZwphc357xjS"
   },
   "source": [
    "We're testing clustering on returns to see if it can provide additional information, wich mean clustering the equity based on their returns, not based on their features. We're using K means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "6-Z00B3K6aIz",
    "outputId": "170a7ee8-decf-458b-9e1b-f89a33d355cb"
   },
   "outputs": [],
   "source": [
    "rendement_cols = [col for col in data.columns if col.startswith(\"r\")]\n",
    "\n",
    "# normalize returns\n",
    "scaler_rendement = RobustScaler()\n",
    "X_rendement_scaled = scaler_rendement.fit_transform(data[rendement_cols])\n",
    "\n",
    "# PCA\n",
    "pca_rendement = PCA()\n",
    "X_rendement_pca = pca_rendement.fit_transform(X_rendement_scaled)\n",
    "cumulative_variance_rendement = np.cumsum(pca_rendement.explained_variance_ratio_)\n",
    "\n",
    "# explained variance and PCA plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "axes[0].scatter(X_rendement_pca[:, 0], X_rendement_pca[:, 1], marker=\"x\")\n",
    "axes[0].set_title(\"PCA on returns\")\n",
    "axes[0].set_xlabel('Principal component 1')\n",
    "axes[0].set_ylabel('Principal component 2')\n",
    "\n",
    "axes[1].plot(range(1, len(cumulative_variance_rendement) + 1),\n",
    "             cumulative_variance_rendement, marker='o')\n",
    "axes[1].set_xlabel('Number of components')\n",
    "axes[1].set_ylabel('Cumulative explained variance')\n",
    "axes[1].set_title(\"Cumulative explained variance by component\")\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sQmaDeb7CDB"
   },
   "source": [
    "The optimal number of clusters is checked using the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "P84e7bti6k59",
    "outputId": "d0326718-2f60-4958-e3e2-6254da0cbf7a"
   },
   "outputs": [],
   "source": [
    "inertia_rendement = []\n",
    "K_rendement = range(1, 11)\n",
    "for k in K_rendement:\n",
    "    kmeans_rendement = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_rendement.fit(X_rendement_pca[:, :2])\n",
    "    inertia_rendement.append(kmeans_rendement.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_rendement, inertia_rendement, marker='o')\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Inertia intra-cluster\")\n",
    "plt.title(\"Elbow method (KMeans on returns PCA)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uT8RESte6Sr6",
    "outputId": "55407d45-f6aa-4035-dcf9-9917ed4c4d73"
   },
   "outputs": [],
   "source": [
    "# KMEANS with 4 clusters (determined by the elbow method)\n",
    "kmeans_final_rendement = KMeans(n_clusters=4, random_state=42)\n",
    "clusters_rendement = kmeans_final_rendement.fit_predict(X_rendement_pca[:, :2])\n",
    "\n",
    "# visualize clusters\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(x=X_rendement_pca[:, 0], y=X_rendement_pca[:, 1], hue=clusters_rendement, palette='tab10', s=60)\n",
    "plt.title(\"KMeans Clusters on PCA of Returns\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YepiGpqJ6yT-",
    "outputId": "8c26be80-55ba-43bd-e017-5f1287d11514"
   },
   "outputs": [],
   "source": [
    "# Count the number of equities per return cluster\n",
    "compter_rendement = pd.Series(clusters_rendement).value_counts().sort_index()\n",
    "pourcentage_rendement = (compter_rendement / compter_rendement.sum()) * 100\n",
    "\n",
    "# Create result DataFrame\n",
    "resultat_rendement = pd.DataFrame({'Cluster': compter_rendement.index,\n",
    "    'Number of equities in cluster': compter_rendement.values,\n",
    "    'Percentage (%)': pourcentage_rendement.round(2).values})\n",
    "\n",
    "resultat_rendement = resultat_rendement.set_index(\"Cluster\")\n",
    "\n",
    "print(\"\\nDistribution of return clusters\")\n",
    "print(resultat_rendement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BapCmKoBvkCs"
   },
   "source": [
    "- We find 4 clusters, just like when we clustered the equity by their features. It's therefore consistent.\n",
    "- We add this classification as a feature of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIamlMT-wFra"
   },
   "outputs": [],
   "source": [
    "data['cluster_rendement'] = clusters_rendement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA1v_aMv3tt-"
   },
   "source": [
    "# Normalization, separation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYt84hyXC7uu"
   },
   "source": [
    "- Separate the data to have 80% training and 20% validation (called testing here).\n",
    "- We ensure that no equity from the test is reflected in the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKzmp7D230KS"
   },
   "outputs": [],
   "source": [
    "# group by equity\n",
    "equity_target = (data.groupby(\"equity\")[\"reod\"]\n",
    "  .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "  .reset_index())\n",
    "\n",
    "# split in 80% train + 20%test, with no similar equity between the two sets\n",
    "equities_train, equities_test = train_test_split(equity_target,\n",
    "    test_size=0.2, random_state=42, stratify=equity_target[\"reod\"])\n",
    "\n",
    "train = data[data[\"equity\"].isin(equities_train[\"equity\"])].copy()\n",
    "test = data[data[\"equity\"].isin(equities_test[\"equity\"])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kug7XtfCFBoh"
   },
   "source": [
    "- We normalize (in both the train and test) our features and returns, using RobustScaler, which allows us to avoid overly compressing the thick tails of the distribution (so as not to lose the economic information that the extreme values ​​provide us).\n",
    "- We graphically represent the distribution of returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsGovl2oFmEx"
   },
   "outputs": [],
   "source": [
    "features_normalisation = ['r0', 'r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7', 'r8',\n",
    "       'r9', 'r10', 'r11', 'r12', 'r13', 'r14', 'r15', 'r16', 'r17', 'r18',\n",
    "       'r19', 'r20', 'r21', 'r22', 'r23', 'r24', 'r25', 'r26', 'r27', 'r28',\n",
    "       'r29', 'r30', 'r31', 'r32', 'r33', 'r34', 'r35', 'r36', 'r37', 'r38',\n",
    "       'r39', 'r40', 'r41', 'r42', 'r43', 'r44', 'r45', 'r46', 'r47', 'r48',\n",
    "       'r49', 'r50', 'r51', 'r52', 'mean_return',\n",
    "       'median_return', 'std_return', 'var_return', 'range_return',\n",
    "       'trend_last_5', 'trend_first_5', 'trend_diff', 'momentum_10',\n",
    "       'momentum_20', 'acceleration', 'volatility_ratio', 'day_sin', 'day_cos',\n",
    "       'day_bin', 'Moyenne mobile 5d', 'Moyenne mobile 10d', 'Vol mobile 5d',\n",
    "       'Vol mobile 10d', 'Momentum mobile 5d', 'Momentum mobile 10d','Vol ratio 10d',]\n",
    "scaler = RobustScaler()\n",
    "train[features_normalisation] = scaler.fit_transform(train[features_normalisation])\n",
    "test[features_normalisation] = scaler.transform(test[features_normalisation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "krR06arn5kLf",
    "outputId": "e0a43011-f422-471e-d8dd-beb65f793bbc"
   },
   "outputs": [],
   "source": [
    "rendement_cols = [col for col in train.columns if col.startswith(\"r\") and col != \"reod\"]\n",
    "rendements_norm = train[rendement_cols].stack().dropna()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(rendements_norm, bins=200, kde=False)\n",
    "plt.yscale('log')  # log scale\n",
    "plt.title(\"Distribution of returns\")\n",
    "plt.xlabel(\"Normalized returns\")\n",
    "plt.ylabel(\"Frequency (log)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDAbQNo6LkZx"
   },
   "source": [
    "The distribution of returns always has thick tails and negative skewness (skewness < 0), meaning that there is a higher frequency of sharp declines than sharp increases. Normalizing does not erase the characteristics of our distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7028-keGaUe"
   },
   "outputs": [],
   "source": [
    "# Define the target variable (y) and features (X)\n",
    "train_x = train.drop(columns=['reod'])  # All columns except the target\n",
    "train_y = train['reod']  # Target\n",
    "\n",
    "test_x = test.drop(columns=['reod'])  # All columns except the target\n",
    "test_y = test['reod']  # Target\n",
    "\n",
    "train_x = train_x.sample(frac=0.1, random_state=42)  # 10% of the data\n",
    "train_y = train_y.loc[train_x.index]\n",
    "\n",
    "train_x.fillna(train_x.median(), inplace=True)  # Fill missing values with median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yOJkqhcIorm"
   },
   "outputs": [],
   "source": [
    "# Shift classes by `+1` to avoid negative values\n",
    "train_y = train_y + 1\n",
    "test_y= test_y + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPWpYhUKsZGb"
   },
   "source": [
    "# Training with non supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8AHdy6YsVAX"
   },
   "source": [
    "Out of curiosity, we look at whether the equity clusters we created previously can predict the reod variable well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj7KOsb3sVAX",
    "outputId": "79f12429-c4d4-407a-ebf9-4dcae1a92ac0"
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(train_x)\n",
    "\n",
    "# Clustering on the training set\n",
    "n_clusters = 7\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters_train = kmeans.fit_predict(X_train_pca)\n",
    "\n",
    "# Associate to each cluster the mean of the target 'reod'\n",
    "df_train = pd.DataFrame(X_train_pca, index=train_x.index)\n",
    "df_train['cluster'] = clusters_train\n",
    "df_train['reod'] = train_y\n",
    "map_reod = df_train.groupby('cluster')['reod'].mean().to_dict()\n",
    "\n",
    "# Apply on the test set\n",
    "X_test_pca = pca.transform(test_x)\n",
    "clusters_test = kmeans.predict(X_test_pca)\n",
    "df_test = pd.DataFrame(X_test_pca, index=test_x.index)\n",
    "df_test['cluster'] = clusters_test\n",
    "df_test['reod'] = test_y\n",
    "df_test['reod_pred_continu'] = df_test['cluster'].map(map_reod)\n",
    "\n",
    "def to_class(x):\n",
    "    if x <= 0.5:\n",
    "        return 0\n",
    "    elif 1.5 > x > 0.5:\n",
    "        return 1\n",
    "    elif x >= 1.5:\n",
    "        return 2\n",
    "\n",
    "df_test['reod_pred'] = df_test['reod_pred_continu'].apply(to_class)\n",
    "\n",
    "# Evaluation\n",
    "y_true = df_test['reod'].astype(int)\n",
    "y_pred = df_test['reod_pred'].astype(int)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy of the 'KMeans + cluster mean' model (test set): {acc:.4f}\")\n",
    "print(df_test[['reod', 'reod_pred', 'cluster']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COa52IVdsVAY"
   },
   "source": [
    "The accuracy obtained with equity clusters isn't very good. We still keep the feature created through clustering in our dataset, as it might be useful for supervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0F8nCsrHvAy"
   },
   "source": [
    "# Selection of the supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xbv09kz9H1gP"
   },
   "source": [
    "Testing several classification models on the training data to find the one that will maximize the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xhE7BYti5nP"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_x, train_y, test_x, test_y, cv=5):\n",
    "    # Train the model\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(test_x)\n",
    "\n",
    "    # Performance evaluation (classification)\n",
    "    accuracy = accuracy_score(test_y, y_pred)\n",
    "    class_report = classification_report(test_y, y_pred, output_dict=True)\n",
    "\n",
    "    # Cross-validation with \"accuracy\" scoring\n",
    "    cv_scores = sk.model_selection.cross_val_score(model, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "    mean_cv_acc = cv_scores.mean()  # Mean accuracy from cross-validation\n",
    "\n",
    "    result = {\"Model\": model.__class__.__name__,\n",
    "        \"Test Accuracy\": accuracy,\n",
    "        \"Mean CV Accuracy\": mean_cv_acc,\n",
    "        \"Classification Report\": class_report}\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Mean CV Accuracy: {mean_cv_acc:.4f}\\n\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaR6kMjxnvNc",
    "outputId": "660e053d-da52-47cd-ea84-3482f9f92fb4"
   },
   "outputs": [],
   "source": [
    "# List of models to test\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"saga\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVC\": SVC(kernel=\"rbf\", C=1.0, class_weight=\"balanced\"),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Test each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating model: {name}\")\n",
    "    result = evaluate_model(model, train_x, train_y, test_x, test_y)\n",
    "    results[name] = result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQbzJO5gP-dC"
   },
   "source": [
    "- Xg Boost : Accuracy: 0.4970, Mean CV Accuracy: 0.4979\n",
    "- Logistic regression : Accuracy: 0.4166, Mean CV Accuracy: 0.4179\n",
    "- Decision Tree : Accuracy: 0.3882, Mean CV Accuracy: 0.3842\n",
    "- Random Forest : Accuracy: 0.444, Mean CV Accuracy: 0.4447\n",
    "- Gradient Boosting : Accuracy: 0.4764, Mean CV Accuracy: 0.4771\n",
    "- SVC : Accuracy: 0.3246, Mean CV Accuracy: 0.3307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 3-class classification task (where a random prediction would have a 33% chance of being correct), the tested models exhibited varied performance.\n",
    "\n",
    "- XGBoost yielded the best results with an accuracy of 49.7%. This indicates that it successfully captured interesting structure in the data. This score is significantly higher than chance.\n",
    "\n",
    "- Gradient Boosting (47.6%) and Random Forest (44.4%) also performed quite well, confirming that ensemble models are well-suited to handling complex and potentially noisy data.\n",
    "\n",
    "- Logistic regression, with 41.7% accuracy, remains adequate but limited, as it relies on linearity assumptions that may not be appropriate for our data.\n",
    "\n",
    "- The decision tree is not good.\n",
    "\n",
    "- SVC achieved only 32.5%: it is the lowest-performing model, and especially the longest to train.\n",
    "\n",
    "The XGBoost model predicts values ​​with the highest accuracy, so we will choose this model for our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT3TYsU_mVqO"
   },
   "source": [
    "Now, we optimize the hyperparameters of the XGBoost model, using Optuna. Instead of testing randomly (GridSearch), Optuna takes into account previous trials to refine the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rXxplqb-tZf-",
    "outputId": "f4244348-6041-4deb-9910-a57a579b8e8e"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10)}\n",
    "\n",
    "    model = xgb.XGBClassifier( **params,use_label_encoder=False,\n",
    "        eval_metric='logloss', random_state=42, verbosity=0)\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    preds = model.predict(test_x)\n",
    "    return accuracy_score(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed seed to always get the same results\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best validation accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the best parameters found by Optuna to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v12JvC9y-PHn",
    "outputId": "cebe4a26-1c33-40ea-dc07-a13b9ba2274d"
   },
   "outputs": [],
   "source": [
    "# Best parameters found by Optuna\n",
    "best_params = {\n",
    "    'n_estimators': 164,\n",
    "    'learning_rate': 0.14447746112718687,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 6,\n",
    "    'subsample': 0.836965827544817,\n",
    "    'colsample_bytree': 0.6185801650879991}\n",
    "\n",
    "# Set seed here as well\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Set seed in the model too\n",
    "model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# Predictions of our output variable\n",
    "y_pred = model.predict(test_x)\n",
    "\n",
    "# Accuracy calculation\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "class_report = classification_report(test_y, y_pred, output_dict=True)\n",
    "\n",
    "# Cross-validation with \"accuracy\" scoring\n",
    "cv_scores = sk.model_selection.cross_val_score(model, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "mean_cv_acc = cv_scores.mean()  # Average accuracy of cross-validation\n",
    "\n",
    "result = {\n",
    "    \"Model\": model.__class__.__name__,\n",
    "    \"Test Accuracy\": accuracy,\n",
    "    \"Mean CV Accuracy\": mean_cv_acc,\n",
    "    \"Classification Report\": class_report\n",
    "}\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}, Mean CV Accuracy: {mean_cv_acc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk6koGm1-PHn"
   },
   "source": [
    "We achieved an accuracy of 0.495 on our test dataset.The accuracy is higher than the one from the data challenge benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the model is overfitted : we check the accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2KwQSxB-PHn",
    "outputId": "8c828f5a-ae35-4f97-cea7-fd258b16bc48"
   },
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_x)\n",
    "train_accuracy = accuracy_score(train_y, train_pred)\n",
    "\n",
    "print(\"Train accuracy :\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23aPeokAk-hx"
   },
   "source": [
    "The accuracy is slightly higher compared to the test, which is a good sign.\n",
    "\n",
    "However, we have more than 80 features, and we wonder if they are all relevant. We will rerun the model, but select only the 40 best features, to see if this improves the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2TLMzARlEFz"
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(**best_params, random_state=42)  # Create a new model with the best parameters\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=train_x.columns)\n",
    "top_k = 40  # Number of features to select\n",
    "selected_features = importances.sort_values(ascending=False).head(top_k).index\n",
    "\n",
    "train_x_selected = train_x[selected_features]\n",
    "test_x_selected = test_x[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RMPN_wSxl1m",
    "outputId": "e6d36cc2-346f-402d-b687-ee20b3de2dfe"
   },
   "outputs": [],
   "source": [
    "models = {    \"XGBoost\": xgb.XGBClassifier(random_state=42)}\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"{name}\")\n",
    "    result = evaluate_model(model, train_x_selected, train_y, test_x_selected, test_y)\n",
    "    results[name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test dataset, accuracy is improved with this model (0.503 vs. 0.495), although this remains slight. We check again on our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTqAIvIOyTpv",
    "outputId": "6e4fde95-43bb-45e6-8c79-14871446a8bc"
   },
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_x_selected)\n",
    "train_accuracy = accuracy_score(train_y, train_pred)\n",
    "\n",
    "print(\"Accuracy on Train :\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain an accuracy of 0.74, which is high and presents a risk of overfitting. This may be due to the fact that we reduced the number of features. We will optimize the hyperparameters to try to recalibrate the model and correct the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10) }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params,use_label_encoder=False,\n",
    "        eval_metric='logloss', random_state=42,verbosity=0  )\n",
    "\n",
    "    model.fit(train_x_selected, train_y)\n",
    "    preds = model.predict(test_x_selected)\n",
    "    return accuracy_score(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "# optuna for the hyperparameters optimisation\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best parameters :\", study.best_params)\n",
    "print(\"Best accuracy on vaidation set :\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHr104QHyFdz",
    "outputId": "4dbecb2a-6bfd-45f1-9a45-17922e202195"
   },
   "outputs": [],
   "source": [
    "# Best parameters found by Optuna\n",
    "best_params = {\n",
    "    'n_estimators': 164,\n",
    "    'learning_rate': 0.14447746112718687,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 6,\n",
    "    'subsample': 0.836965827544817,\n",
    "    'colsample_bytree': 0.6185801650879991}\n",
    "\n",
    "# Seed\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Create the model with the best parameters\n",
    "model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model.fit(train_x_selected, train_y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(test_x_selected)\n",
    "\n",
    "# Accuracy computation\n",
    "accuracy = accuracy_score(test_y, y_pred)\n",
    "class_report = classification_report(test_y, y_pred, output_dict=True)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = sk.model_selection.cross_val_score(model, train_x_selected, train_y, cv=cv, scoring='accuracy')\n",
    "mean_cv_acc = cv_scores.mean()  # Mean accuracy from cross-validation\n",
    "\n",
    "result = { \"Model\": model.__class__.__name__,\n",
    "    \"Test Accuracy\": accuracy,\n",
    "    \"Mean CV Accuracy\": mean_cv_acc,\n",
    "    \"Classification Report\": class_report}\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}, Mean CV Accuracy: {mean_cv_acc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_x_selected)\n",
    "train_accuracy = accuracy_score(train_y, train_pred)\n",
    "\n",
    "print(\"Accuracy on Train :\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model with XgBoost, applied only to the best features and with optimized hyperparameters, achieves an accuracy of 0.57 on the train set versus 0.502 on the test set.\n",
    "\n",
    "The accuracy for the test set is significantly higher than the benchmark. Moreover, the accuracy on the train set is very close to the accuracy of the test set, so we were able to address the overfitting issue through hyperparameter calibration. We believe this model is the best we have tested, and this is the one we are retaining."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qpfkX8skqOFE",
    "omrllEIOtATg",
    "Skt2HZHatATn",
    "RO7vsC77tATo",
    "hFtwMCzg2lC-",
    "DidmXHhZtUuB",
    "3e7dNDmM7xjR",
    "aA1v_aMv3tt-",
    "SPWpYhUKsZGb"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
